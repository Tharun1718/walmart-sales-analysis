{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2131795-b403-4c8f-9804-6683483c9ac9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "663f8152-431c-460f-bcec-a1aaffc5680b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to create Spark session\n",
    "def create_spark_session(app_name=\"WalmartSalesDataAnalysis\"):\n",
    "    return SparkSession.builder.appName(app_name).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6de42b60-06c3-42d2-a369-ac12be7efb21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to read data from files\n",
    "def read_data(spark_session, customers_path, salestxns_path):\n",
    "    # Define schema for customers\n",
    "    customers_schema = StructType([\n",
    "        StructField(\"customer_id\", IntegerType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"state\", StringType(), True),\n",
    "        StructField(\"zipcode\", StringType(), True)\n",
    "    ])\n",
    "    # Define schema for salestxns\n",
    "    salestxns_schema = StructType([\n",
    "        StructField(\"sales_txn_id\", IntegerType(), True),\n",
    "        StructField(\"category_id\", IntegerType(), True),\n",
    "        StructField(\"category_name\", StringType(), True),\n",
    "        StructField(\"product_id\", IntegerType(), True),\n",
    "        StructField(\"product_name\", StringType(), True),\n",
    "        StructField(\"price\", DoubleType(), True),\n",
    "        StructField(\"quantity\", IntegerType(), True),\n",
    "        StructField(\"customer_id\", IntegerType(), True)\n",
    "    ])\n",
    "    # Read customers and salestxns files with defined schema\n",
    "    customers_df = spark_session.read.option(\"delimiter\", \"\\t\").schema(customers_schema).csv(customers_path)\n",
    "    salestxns_df = spark_session.read.option(\"delimiter\", \"\\t\").schema(salestxns_schema).csv(salestxns_path)\n",
    "    return customers_df, salestxns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0ea813f-713d-4737-8d88-f243dfbd0846",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to join dataframes and perform aggregation\n",
    "def join_and_aggregate_data(customers_df, salestxns_df):\n",
    "    # Join salestxns with customers to add customer details\n",
    "    joined_df = salestxns_df.join(customers_df, on=\"customer_id\")\n",
    "    # Group by customer id, product id, and aggregate quantity\n",
    "    aggregated_df = joined_df.groupBy(\"customer_id\", \"name\", \"product_id\", \"product_name\", \"price\").agg({\"quantity\": \"sum\"}).withColumnRenamed(\"sum(quantity)\", \"total_quantity\")\n",
    "    return aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cb96aed-ed03-48fc-83da-bcd0ae5e227e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to calculate total amount\n",
    "def calculate_total_amount(aggregated_df):\n",
    "    # Calculate total amount using price and total quantity, format to two decimal places\n",
    "    result_df = aggregated_df.withColumn(\"total_amount\", format_number(aggregated_df[\"price\"] * aggregated_df[\"total_quantity\"], 2))\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6b9d8de-7f59-4402-8443-3a7bfa18221b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to select relevant columns\n",
    "def select_relevant_columns(result_df):\n",
    "    # Select relevant columns for display\n",
    "    return result_df.select(\"customer_id\", \"name\", \"product_id\", \"product_name\", \"price\", \"total_quantity\", \"total_amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d973833-e038-4ff5-8a2a-0a2a8ac5ada6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to write output to CSV\n",
    "def write_output_to_csv(result_df, output_path):\n",
    "    # Convert PySpark DataFrame to Pandas DataFrame and write to CSV\n",
    "    output_df = result_df.toPandas()\n",
    "    output_df.to_csv(output_path, index=False)\n",
    "    print(\"File uploaded successfully to: \", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1123b11c-ee75-4f6a-a7fa-27cf909dde4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to stop Spark session\n",
    "def stop_spark_session(spark_session):\n",
    "    spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d564e1f-c020-468f-98dd-fd5f4d7bfafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded successfully to:  data\\output.csv\n"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "def main():\n",
    "    # Create Spark session\n",
    "    spark = create_spark_session()\n",
    "    # Paths to input and output files\n",
    "    customers_path = \"data\\\\customers.txt\"\n",
    "    salestxns_path = \"data\\\\salestxns.txt\"\n",
    "    output_path = \"data\\\\output.csv\"\n",
    "    \n",
    "    # Read data from files\n",
    "    customers_df, salestxns_df = read_data(spark, customers_path, salestxns_path)\n",
    "    # Join dataframes and perform aggregation\n",
    "    aggregated_df = join_and_aggregate_data(customers_df, salestxns_df)\n",
    "    # Calculate total amount\n",
    "    result_df = calculate_total_amount(aggregated_df)\n",
    "    # Select relevant columns\n",
    "    result_df = select_relevant_columns(result_df)\n",
    "    # Write output to CSV\n",
    "    write_output_to_csv(result_df, output_path)\n",
    "    # Stop Spark session\n",
    "    stop_spark_session(spark)\n",
    "\n",
    "# Entry point of the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0c89ea-cbc7-4857-828b-3b15731e1bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
